{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from scipy.optimize import linprog\n",
    "from qpsolvers import solve_qp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import StochasticGhost1\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "xd, yd = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "xd = (xd/255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "\n",
    "    # For now the input data is passed as init parameters\n",
    "    def __init__(self, layer_sizes, itrain, otrain, ival, oval):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # Create a list of linear layers based on layer_sizes\n",
    "        self.itrain = itrain\n",
    "        self.otrain = otrain\n",
    "        self.ival = ival\n",
    "        self.oval = oval\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.input = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.sigmoid(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        compute loss function\n",
    "        \"\"\"\n",
    "        # L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "        L_sum = 0.5*torch.sum(torch.square(Y - Y_hat))\n",
    "\n",
    "        m = Y.shape[0]\n",
    "        L = (1./m) * L_sum\n",
    "\n",
    "        return L\n",
    "    \n",
    "    def obj_fun(self, params, minibatch):\n",
    "        model_parameters = list(self.parameters())\n",
    "        # with torch.no_grad():\n",
    "        x = self.itrain\n",
    "        y = self.otrain\n",
    "        samples = np.random.choice(len(y), minibatch, replace=False)\n",
    "        for i in range(len(params)):\n",
    "            model_parameters[i].data = torch.Tensor(params[i]) \n",
    "        #self.compute_loss(fval, y[minibatch])\n",
    "        fval = self.compute_loss(self.forward(x[samples, :]), y[samples])\n",
    "        return fval.item()\n",
    "    \n",
    "    def obj_grad(self, params, minibatch):\n",
    "        #f_grad = {}\n",
    "        fgrad = []\n",
    "        x = self.itrain\n",
    "        y = self.otrain\n",
    "        samples = np.random.choice(len(y), minibatch, replace=False)\n",
    "        #with torch.no_grad():\n",
    "        for name, param in self.named_parameters():\n",
    "            #print(name)\n",
    "            fgrad.append(torch.autograd.grad(\n",
    "                self.compute_loss(self.forward(x[samples, :]), y[samples]), param, retain_graph=True)[0].view(-1))\n",
    "        fgrad = torch.cat(fgrad, dim=0)\n",
    "        return fgrad\n",
    "    \n",
    "    def conf(self, params, minibatch):\n",
    "        conf_val = None\n",
    "        x_val = self.ival\n",
    "        y_val = self.oval\n",
    "        samples = np.random.choice(len(y_val), minibatch, replace=False)\n",
    "        #print(y_val.shape)\n",
    "        #print(x_val.shape)\n",
    "        conf_val = self.forward(x_val[minibatch, :])\n",
    "        conf_val = self.compute_loss(\n",
    "            self.forward(x_val[samples, :]), y_val[samples])\n",
    "        \"\"\"\n",
    "        TODO: Compute\n",
    "        \n",
    "        \"\"\"\n",
    "        return conf_val.item()\n",
    "    \n",
    "    def conJ(self, params, minibatch):\n",
    "        x_val = self.ival\n",
    "        y_val = self.oval\n",
    "        cgrad = []\n",
    "        samples = np.random.choice(len(y_val), minibatch, replace=False)\n",
    "        #with torch.no_grad():\n",
    "        for name, param in self.named_parameters():\n",
    "            #print(name)\n",
    "            cgrad.append(torch.autograd.grad(\n",
    "                self.compute_loss(self.forward(x_val[samples, :]), y_val[samples]), param, retain_graph=True)[0].view(-1))\n",
    "        \n",
    "        cgrad = torch.cat(cgrad, dim=0)\n",
    "        \"\"\"\"\n",
    "        TODO: Compute\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"I am the grad tensor and I am this long: \", len(cgrad))\n",
    "        return cgrad\n",
    "    \n",
    "\n",
    "\n",
    "    # def solvesubp(fgrad, cval, cgrad, kap, beta, tau, n):\n",
    "    #     P = tau*np.eye(n)\n",
    "\n",
    "    #     return solve_qp(P, fgrad.numpy().reshape((n,)), cgrad.numpy().reshape((1, n)), np.array([(kap-cval.numpy())]), np.zeros((0, n)), np.zeros((0,)), -beta*np.ones((n,)), beta*np.ones((n,)))\n",
    "\n",
    "    # def computekappa(self, cval, cgrad, rho, lamb, N):\n",
    "    #     obj = np.concatenate(([1.], np.zeros((N,))))\n",
    "    #     Aubt = np.concatenate((([-1.]), cgrad.numpy()))\n",
    "    #     Aubt = Aubt.reshape(1, N+1)\n",
    "    #     res = linprog(c=obj, A_ub=Aubt,\n",
    "    #                   b_ub=[-cval.numpy()], bounds=(-rho, rho))\n",
    "\n",
    "    #     return ((1-lamb)*max(0, cval.numpy())+lamb*max(0, res.fun))\n",
    "\n",
    "    # def backward(self, loss, constraint, param_dict):\n",
    "\n",
    "    #     f_grad = {}\n",
    "    #     fgrad = []\n",
    "    #     cgrad = []\n",
    "    #     c_grad = {}\n",
    "    #     param_tot = sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    #     for name, param in self.named_parameters():\n",
    "    #         print(name)\n",
    "    #         f_grad[name] = torch.autograd.grad(\n",
    "    #             loss, param, retain_graph=True)[0]\n",
    "    #         fgrad.append(torch.cat(fgrad, f_grad[name].view(-1)))\n",
    "\n",
    "    #     # Custom backward pass to add regularization term\n",
    "    #     for name, param in self.named_parameters():\n",
    "    #         print(name)\n",
    "    #         # Assuming param is a weight matrix\n",
    "    #         c_grad[name] = torch.autograd.grad(\n",
    "    #             constraint, param, retain_graph=True)[0]\n",
    "    #         cgrad.append(torch.cat(cgrad, f_grad[name].view(-1)))\n",
    "\n",
    "    #     kap = self.computekappa(\n",
    "    #         constraint, cgrad, param_dict[\"rho\"], param_dict[\"lamb\"], param_tot)\n",
    "    #     dsol = self.solvesubp(fgrad, constraint, cgrad, kap,\n",
    "    #                           param_dict[\"rho\"], param_dict[\"lamb\"], param_tot)\n",
    "    #     return dsol\n",
    "\n",
    "    # def update(self, grad, gamma):\n",
    "    #     start = 0\n",
    "    #     with torch.no_grad():\n",
    "    #         for name, param in self.named_parameters():\n",
    "    #             print(name)\n",
    "    #             end = start + param.numel()\n",
    "    #             param += gamma*grad[start:end].view_as(param)\n",
    "    #             start = end\n",
    "\n",
    "    # def compute_loss(self, Y, Y_hat):\n",
    "    #     \"\"\"\n",
    "    #     compute loss function\n",
    "    #     \"\"\"\n",
    "    #     # L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "    #     L_sum = 0.5*np.sum(np.square(Y - Y_hat))\n",
    "\n",
    "    #     m = Y.shape[0]\n",
    "    #     L = (1./m) * L_sum\n",
    "\n",
    "    #     return L\n",
    "    \n",
    "\n",
    "    # def getBatches(Nbatch, NSamp):\n",
    "    #     mbatch1 = np.random.choice(Nbatch, 1, replace=False)\n",
    "    #     mbatch2 = np.random.choice(Nbatch, 2**NSamp, replace=False)\n",
    "    #     mbatch3 = np.random.choice(Nbatch, 2**NSamp, replace=False)\n",
    "    #     mbatch4 = np.random.choice(Nbatch, 2**(NSamp+1), replace=False)\n",
    "    #     # trainT = data_utils.TensorDataset(torch.Tensor(trainT_X.values), torch.Tensor(trainT_Y.values))\n",
    "    #     # train_dataloaderT = data_utils.DataLoader(trainT, batch_size=18, shuffle=True)\n",
    "    #     return [mbatch1, mbatch2, mbatch3, mbatch4]\n",
    "\n",
    "    # def train_and_update(self, xd, yd):\n",
    "    #     X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    #         xd, yd, test_size=0.3, random_state=42)\n",
    "    #     maxiter = 500\n",
    "    #     geomp = 0.7\n",
    "    #     # Example: Input size, hidden sizes, output size\n",
    "    #     param_dict = {\"rho\": 0.8, \"lamb\": 0.5, \"beta\": 10.,\n",
    "    #                   \"tau\": 1., \"geomp\": 0.7, \"maxiter\": 500}\n",
    "\n",
    "    #     geomp = param_dict[\"geomp\"]\n",
    "    #     Nsamp = np.random.geometric(p=geomp)\n",
    "    #     cons_bound = 10.\n",
    "    #     while (2**(Nsamp+1)) > min(len(X_train), len(X_val)):\n",
    "    #         Nsamp = np.random.geometric(p=geomp)\n",
    "\n",
    "    #     mbatcht = self.getBatches(self, len(X_train), Nsamp)\n",
    "    #     mbatchv = self.getBatches(self, len(X_val), Nsamp)\n",
    "\n",
    "    #     # size = len(dataloader.dataset)\n",
    "    #     # model.train()\n",
    "    #     accum_loss = 0\n",
    "    #     dsol = []\n",
    "    #     for i in len(mbatcht):\n",
    "    #         output_t = self.forward(X_train[:, mbatcht[i]])\n",
    "    #         output_v = self.forward(X_val[:, mbatchv[i]])\n",
    "    #         t_loss = self.compute_loss(Y_train[mbatcht[i]], output_t)\n",
    "    #         constraint = output_v-cons_bound\n",
    "    #         dsol.append(self.backward(t_loss, constraint, param_dict))\n",
    "\n",
    "    #     d = dsol[0] + (dsol[3]-0.5*dsol[1]-0.5*dsol[2]) / \\\n",
    "    #         (geomp*((1-geomp)**Nsamp))\n",
    "    #     self.update(d, geomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd = yd.astype(int)\n",
    "y = np.zeros((yd.shape[0],))\n",
    "y[np.where(yd == 1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paramvals(maxiter, beta, rho, lamb, hess, tau, mbsz, numcon, geomp, stepdecay, gammazero, zeta, N, n):\n",
    "    params = {\n",
    "        'maxiter': maxiter,  # number of iterations performed\n",
    "        'beta': beta,  # trust region size\n",
    "        'rho': rho,  # trust region for feasibility subproblem\n",
    "        'lamb': lamb,  # weight on the subfeasibility relaxation\n",
    "        'hess': hess,  # method of computing the Hessian of the QP, options include 'diag' 'lbfgs' 'fisher' 'adamdiag' 'adagraddiag'\n",
    "        'tau': tau,  # parameter for the hessian\n",
    "        'mbsz': mbsz,  # the standard minibatch size, used for evaluating the progress of the objective and constraint\n",
    "        'numcon': numcon,  # number of constraint functions\n",
    "        'geomp': geomp,  # parameter for the geometric random variable defining the number of subproblem samples\n",
    "        'stepdecay': stepdecay, # strategy for step decrease, options include 'dimin' 'stepwise' 'slowdimin' 'constant'\n",
    "        'gammazero': gammazero,  # initial stepsize\n",
    "        'zeta': zeta,  # parameter associated with the stepsize iteration\n",
    "        'N' : N, #Train/val sample size\n",
    "        'n' : n  # Total number of parameters\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_package(root_module):\n",
    "    package_name = root_module.__name__\n",
    "\n",
    "    # get a reference to each loaded module\n",
    "    loaded_package_modules = dict([\n",
    "        (key, value) for key, value in sys.modules.items() \n",
    "        if key.startswith(package_name) and isinstance(value, types.ModuleType)])\n",
    "\n",
    "    # delete references to these loaded modules from sys.modules\n",
    "    for key in loaded_package_modules:\n",
    "        del sys.modules[key]\n",
    "\n",
    "    # load each of the modules again; \n",
    "    # make old modules share state with new modules\n",
    "    for key in loaded_package_modules:\n",
    "        print(\"Loading \", key)\n",
    "        newmodule = __import__(key)\n",
    "        oldmodule = loaded_package_modules[key]\n",
    "        oldmodule.__dict__.clear()\n",
    "        oldmodule.__dict__.update(newmodule.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  StochasticGhost1\n"
     ]
    }
   ],
   "source": [
    "import StochasticGhost1\n",
    "reload_package(StochasticGhost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck!!!! 1321  parameters, Go easy on me, I am just cute little pc.\n",
      "Type of initw: <class 'list'>\n",
      "LOOKIE LOOKIE:::::::::  0.1528610946724247\n",
      "<class 'numpy.ndarray'>\n",
      "I am the grad tensor and I am this long:  1321\n",
      "<class 'numpy.ndarray'>\n",
      "The cgrad shape is: (1321,)\n",
      "The Aubt shape is: (1322,)\n",
      "<class 'numpy.float64'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[-0.         -0.         -0.         ... -0.2797214  -0.24064624\n",
      " -0.55292153]\n",
      "[0.         0.         0.         ... 0.22617571 0.19458051 0.4470785 ]\n",
      "<class 'numpy.ndarray'>\n",
      "I am the grad tensor and I am this long:  1321\n",
      "<class 'numpy.ndarray'>\n",
      "The cgrad shape is: (1321,)\n",
      "The Aubt shape is: (1322,)\n",
      "<class 'numpy.float64'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[0.         0.         0.         ... 0.90470284 0.77832204 1.788314  ]\n",
      "[0.         0.         0.         ... 0.90470284 0.77832204 1.78831398]\n",
      "<class 'numpy.ndarray'>\n",
      "I am the grad tensor and I am this long:  1321\n",
      "<class 'numpy.ndarray'>\n",
      "The cgrad shape is: (1321,)\n",
      "The Aubt shape is: (1322,)\n",
      "<class 'numpy.float64'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[0.         0.         0.         ... 0.39880577 0.34309533 0.788314  ]\n",
      "[0.         0.         0.         ... 0.39880577 0.34309533 0.78831398]\n",
      "<class 'numpy.ndarray'>\n",
      "I am the grad tensor and I am this long:  1321\n",
      "<class 'numpy.ndarray'>\n",
      "The cgrad shape is: (1321,)\n",
      "The Aubt shape is: (1322,)\n",
      "<class 'numpy.float64'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[0.        0.        0.        ... 1.3001553 1.1201077 2.5743608]\n",
      "[0.         0.         0.         ... 1.80940557 1.5566442  3.57662797]\n",
      "Hey, I reached here and calculated the value of d :)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "maxiter = 1\n",
    "layer_sizes = [70, 15, 15, 1]\n",
    "#print(type(xd))\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    torch.tensor(xd.values), torch.tensor(y), test_size=0.3, random_state=42)\n",
    "#print(type(X_train))\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "cons_bound = 10.\n",
    "#X_train = torch.tensor()\n",
    "net = CustomNetwork(layer_sizes, X_train[:, :70], Y_train, X_val[:, :70], Y_val)\n",
    "nn_parameters = list(net.parameters())\n",
    "initw = [param.data for param in nn_parameters]\n",
    "num_param = sum(p.numel() for p in net.parameters())\n",
    "print(\"Fuck!!!!\", num_param,\" parameters, Go easy on me, I am just cute little pc.\")\n",
    "#net.train_and_update(xd, yd)\n",
    "#output_t = net.forward(X_train[:, :70])\n",
    "#output_v = net.forward(X_val[:, :70])\n",
    "#obj = net.compute_loss(Y_train[:70], output_t)\n",
    "#constraint = output_v-cons_bound\n",
    "params = paramvals(maxiter=2, beta=10, rho=0.8, lamb=0.5, hess='diag', tau=1., mbsz=1,\n",
    "                   numcon=1, geomp=0.7, stepdecay='dimin', gammazero=0.1, zeta=0.1, N=X_val.shape[0], n=num_param)\n",
    "#print(params[\"N\"])\n",
    "#print((initw))\n",
    "w, iterfs, itercs = StochasticGhost1.StochasticGhost(net.obj_fun, net.obj_grad, [net.conf], [net.conJ], initw, params)\n",
    "#train_loss_list.append(train_loss)\n",
    "#y_pred = net.forward(X_val)\n",
    "#val_loss = net.compute_loss(Y_val, y_pred)\n",
    "#val_loss_list.append(val_loss)\n",
    "\n",
    "# net = CustomNetwork(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10296745]\n",
      "[0.10296745]\n"
     ]
    }
   ],
   "source": [
    "print(iterfs)\n",
    "print(itercs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4,  ..., 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "yd = yd.astype(int)\n",
    "print(torch.tensor(yd.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
