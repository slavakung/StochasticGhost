{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 11:15:39.564826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-20 11:16:09.654869: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/ot/backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from scipy.optimize import linprog\n",
    "from qpsolvers import solve_qp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import StochasticGhost\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "xd, yd = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "xd = (xd/255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNetwork(nn.Module):\n",
    "\n",
    "    # For now the input data is passed as init parameters\n",
    "    def __init__(self, layer_sizes, itrain, otrain, ival, oval):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # Create a list of linear layers based on layer_sizes\n",
    "        self.itrain = itrain\n",
    "        self.otrain = otrain\n",
    "        self.ival = ival\n",
    "        self.oval = oval\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.input = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.sigmoid(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        compute loss function\n",
    "        \"\"\"\n",
    "        # L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
    "        L_sum = 0.5*torch.sum(torch.square(Y - Y_hat))\n",
    "\n",
    "        m = Y.shape[0]\n",
    "        L = (1./m) * L_sum\n",
    "\n",
    "        return L\n",
    "\n",
    "    def obj_fun(self, params, minibatch):\n",
    "        model_parameters = list(self.parameters())\n",
    "        # with torch.no_grad():\n",
    "        x = self.itrain\n",
    "        y = self.otrain\n",
    "        samples = np.random.choice(len(y), minibatch, replace=False)\n",
    "        for i in range(len(params)):\n",
    "            model_parameters[i].data = torch.Tensor(params[i])\n",
    "        # self.compute_loss(fval, y[minibatch])\n",
    "        fval = self.compute_loss(self.forward(x[samples, :]), y[samples])\n",
    "        return fval.item()\n",
    "\n",
    "    def obj_grad(self, params, minibatch):\n",
    "        # f_grad = {}\n",
    "        fgrad = []\n",
    "        x = self.itrain\n",
    "        y = self.otrain\n",
    "        samples = np.random.choice(len(y), minibatch, replace=False)\n",
    "        # with torch.no_grad():\n",
    "        for name, param in self.named_parameters():\n",
    "            fgrad.append(torch.autograd.grad(\n",
    "                self.compute_loss(self.forward(x[samples, :]), y[samples]), param, retain_graph=True)[0].view(-1))\n",
    "        fgrad = torch.cat(fgrad, dim=0)\n",
    "        return fgrad\n",
    "\n",
    "    def conf(self, params, minibatch):\n",
    "        conf_val = None\n",
    "        #x_val = self.ival\n",
    "        #y_val = self.oval\n",
    "        #samples = np.random.choice(len(y_val), minibatch, replace=False)\n",
    "        params = list(self.parameters())\n",
    "\n",
    "# Extract the weights and biases\n",
    "        W1 = params[0].reshape(-1)\n",
    "        W2 = params[2].reshape(-1)\n",
    "        B1 = params[1].reshape(-1)\n",
    "        B2 = params[3].reshape(-1)\n",
    "\n",
    "# Perform the operations in PyTorch\n",
    "        conf_val = torch.dot(W1, W1) / 4. + torch.dot(W2, W2) / 4. + torch.dot(B1, B1) + torch.dot(B2, B2)\n",
    "        \"\"\"\n",
    "        TODO: Compute\n",
    "        \n",
    "        \"\"\"\n",
    "        return conf_val.item() - 5.\n",
    "\n",
    "    def conJ(self, params, minibatch):\n",
    "        # x_val = self.ival\n",
    "        # y_val = self.oval\n",
    "        # samples = np.random.choice(len(y_val), minibatch, replace=False)\n",
    "        # with torch.no_grad():\n",
    "        #for name, param in self.named_parameters():\n",
    "        # print(name)\n",
    "        params = list(self.parameters())\n",
    "        cgrad = [(params[0]/2.).reshape(-1), 2*params[1].reshape(-1), (params[2]/2.).reshape(-1), 2*params[3].reshape(-1)]\n",
    "\n",
    "        cgrad = torch.cat(cgrad, dim=0)\n",
    "        print(cgrad)\n",
    "        print(cgrad.shape)\n",
    "        \"\"\"\"\n",
    "        TODO: Compute\n",
    "\n",
    "        \"\"\"\n",
    "        return cgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yd = yd.astype(int)\n",
    "y = np.zeros((yd.shape[0],))\n",
    "y[np.where(yd == 1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paramvals(maxiter, beta, rho, lamb, hess, tau, mbsz, numcon, geomp, stepdecay, gammazero, zeta, N, n):\n",
    "    params = {\n",
    "        'maxiter': maxiter,  # number of iterations performed\n",
    "        'beta': beta,  # trust region size\n",
    "        'rho': rho,  # trust region for feasibility subproblem\n",
    "        'lamb': lamb,  # weight on the subfeasibility relaxation\n",
    "        'hess': hess,  # method of computing the Hessian of the QP, options include 'diag' 'lbfgs' 'fisher' 'adamdiag' 'adagraddiag'\n",
    "        'tau': tau,  # parameter for the hessian\n",
    "        'mbsz': mbsz,  # the standard minibatch size, used for evaluating the progress of the objective and constraint\n",
    "        'numcon': numcon,  # number of constraint functions\n",
    "        'geomp': geomp,  # parameter for the geometric random variable defining the number of subproblem samples\n",
    "        # strategy for step decrease, options include 'dimin' 'stepwise' 'slowdimin' 'constant'\n",
    "        'stepdecay': stepdecay,\n",
    "        'gammazero': gammazero,  # initial stepsize\n",
    "        'zeta': zeta,  # parameter associated with the stepsize iteration\n",
    "        'N': N,  # Train/val sample size\n",
    "        'n': n  # Total number of parameters\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_package(root_module):\n",
    "    package_name = root_module.__name__\n",
    "\n",
    "    # get a reference to each loaded module\n",
    "    loaded_package_modules = dict([\n",
    "        (key, value) for key, value in sys.modules.items()\n",
    "        if key.startswith(package_name) and isinstance(value, types.ModuleType)])\n",
    "\n",
    "    # delete references to these loaded modules from sys.modules\n",
    "    for key in loaded_package_modules:\n",
    "        del sys.modules[key]\n",
    "\n",
    "    # load each of the modules again;\n",
    "    # make old modules share state with new modules\n",
    "    for key in loaded_package_modules:\n",
    "        print(\"Loading \", key)\n",
    "        newmodule = __import__(key)\n",
    "        oldmodule = loaded_package_modules[key]\n",
    "        oldmodule.__dict__.clear()\n",
    "        oldmodule.__dict__.update(newmodule.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  StochasticGhost\n"
     ]
    }
   ],
   "source": [
    "import StochasticGhost\n",
    "reload_package(StochasticGhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOKIE LOOKIE:::::::::  -3.5958938598632812\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0969, -0.1056, -0.0298],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[-0.         -0.         -0.         ... -0.0035899  -0.00339783\n",
      " -0.00687086]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09693245 -0.10561271\n",
      " -0.02977518]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0969, -0.1056, -0.0298],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.         ... -0.0071798  -0.00679565\n",
      " -0.01374172]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09693245 -0.10561271\n",
      " -0.02977518]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0969, -0.1056, -0.0298],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.         ... -0.0071798  -0.00679565\n",
      " -0.01374172]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09693245 -0.10561271\n",
      " -0.02977518]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0969, -0.1056, -0.0298],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.         ... -0.01435968 -0.01359138\n",
      " -0.0274836 ]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09693245 -0.10561271\n",
      " -0.02977518]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0988, -0.1038, -0.0153],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[0.         0.         0.         ... 0.01438065 0.01360633 0.02751913]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09882144 -0.10382479\n",
      " -0.01531351]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0988, -0.1038, -0.0153],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.         ... -0.49380788 -0.4672188\n",
      " -0.9449617 ]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09882144 -0.10382479\n",
      " -0.01531351]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0988, -0.1038, -0.0153],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.         ... -0.49380788 -0.4672188\n",
      " -0.9449617 ]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09882144 -0.10382479\n",
      " -0.01531351]\n",
      "<class 'numpy.ndarray'>\n",
      "tensor([ 0.0316,  0.0244, -0.0472,  ...,  0.0988, -0.1038, -0.0153],\n",
      "       grad_fn=<CatBackward0>)\n",
      "torch.Size([1081])\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'float'> <class 'numpy.ndarray'> <class 'numpy.float64'> <class 'numpy.ndarray'>\n",
      "[ 0.          0.          0.         ... -0.46504667 -0.44000623\n",
      " -0.88992363]\n",
      "[ 0.03157401  0.02438785 -0.04720455 ...  0.09882144 -0.10382479\n",
      " -0.01531351]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/qpsolvers/conversions/ensure_sparse_matrices.py:38: UserWarning: Converted P to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build P as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n",
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/qpsolvers/conversions/ensure_sparse_matrices.py:38: UserWarning: Converted G to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build G as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n",
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/qpsolvers/conversions/ensure_sparse_matrices.py:38: UserWarning: Converted A to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build A as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "maxiter = 1\n",
    "layer_sizes = [70, 15, 1]\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    torch.tensor(xd.values), torch.tensor(y), test_size=0.3, random_state=42)\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "cons_bound = 10.\n",
    "# X_train = torch.tensor()\n",
    "net = CustomNetwork(\n",
    "    layer_sizes, X_train[:, :70], Y_train, X_val[:, :70], Y_val)\n",
    "nn_parameters = list(net.parameters())\n",
    "initw = [param.data for param in nn_parameters]\n",
    "num_param = sum(p.numel() for p in net.parameters())\n",
    "# net.train_and_update(xd, yd)\n",
    "# output_t = net.forward(X_train[:, :70])\n",
    "# output_v = net.forward(X_val[:, :70])\n",
    "# obj = net.compute_loss(Y_train[:70], output_t)\n",
    "# constraint = output_v-cons_bound\n",
    "params = paramvals(maxiter=2, beta=10, rho=0.8, lamb=0.5, hess='diag', tau=1., mbsz=1,\n",
    "                   numcon=1, geomp=0.7, stepdecay='dimin', gammazero=0.1, zeta=0.1, N=X_val.shape[0], n=num_param)\n",
    "w, iterfs, itercs = StochasticGhost.StochasticGhost(\n",
    "    net.obj_fun, net.obj_grad, [net.conf], [net.conJ], initw, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
