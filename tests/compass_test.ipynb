{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 17:52:44.898341: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 17:53:05.430937: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/harsh/anaconda3/envs/ghost/lib/python3.10/site-packages/ot/backend.py:2998: UserWarning: To use TensorflowBackend, you need to activate the tensorflow numpy API. You can activate it by running: \n",
      "from tensorflow.python.ops.numpy_ops import np_config\n",
      "np_config.enable_numpy_behavior()\n",
      "  register_backend(TensorflowBackend())\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from scipy.optimize import linprog\n",
    "from qpsolvers import solve_qp\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.autograd import Function\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import StochasticGhost\n",
    "import importlib\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"compas-scores-two-years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data[['age', 'c_charge_degree', 'race', 'age_cat', 'score_text', 'sex', 'priors_count',\n",
    "               'days_b_screening_arrest', 'decile_score', 'is_recid', 'two_year_recid', 'c_jail_in', 'c_jail_out']]\n",
    "df = df[(df['days_b_screening_arrest'] <= 30) & (df['days_b_screening_arrest'] >= -30) &\n",
    "        (df['is_recid'] != -1) & (df['c_charge_degree'] != \"O\") & (df['score_text'] != 'N/A')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length_of_stay'] = pd.to_datetime(df['c_jail_out']) - pd.to_datetime(df['c_jail_in'])\n",
    "df['length_of_stay'] = df['length_of_stay'].dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length_of_stay and correlation\n",
    "correlation = df['length_of_stay'].corr(df['decile_score'])\n",
    "print(f\"Correlation between length_of_stay and decile_score: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_distribution = df['race'].value_counts(normalize=True) * 100\n",
    "print(race_distribution)\n",
    "\n",
    "# Summary of score_text\n",
    "print(df['score_text'].describe())\n",
    "\n",
    "# Cross-tabulation of sex and race\n",
    "sex_race_cross_tab = pd.crosstab(df['sex'], df['race'])\n",
    "print(sex_race_cross_tab)\n",
    "\n",
    "# Summary of sex\n",
    "print(df['sex'].describe())\n",
    "\n",
    "# Percentage of two_year_recid == 1\n",
    "recid_percentage = len(df[df['two_year_recid'] == 1]) / len(df) * 100\n",
    "print(f\"Percentage of two_year_recid == 1: {recid_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Plot for African-American\n",
    "pblack = df[df['race'] == \"African-American\"]['decile_score'].value_counts(\n",
    ").sort_index().plot(kind='bar', ax=axes[0])\n",
    "pblack.set_title(\"Black Defendant's Decile Scores\")\n",
    "pblack.set_xlabel('Decile Score')\n",
    "pblack.set_ylabel('Count')\n",
    "\n",
    "# Plot for Caucasian\n",
    "pwhite = df[df['race'] == \"Caucasian\"]['decile_score'].value_counts(\n",
    ").sort_index().plot(kind='bar', ax=axes[1])\n",
    "pwhite.set_title(\"White Defendant's Decile Scores\")\n",
    "pwhite.set_xlabel('Decile Score')\n",
    "pwhite.set_ylabel('Count')\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_needed = df[(df['race'] == 'Caucasian') | (df['race'] =='African-American')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizing\n",
    "df_needed['crime_code'] = pd.Categorical(df_needed['c_charge_degree']).codes\n",
    "df_needed['age_code'] = pd.Categorical(df_needed['age_cat']).codes\n",
    "df_needed['race_code'] = pd.Categorical(df_needed['race']).codes\n",
    "df_needed['gender_code'] = pd.Categorical(df_needed['sex']).codes\n",
    "df_needed['score_code'] = pd.Categorical(df_needed['score_text']).codes\n",
    "df_needed['charge_degree_code'] = pd.Categorical(\n",
    "    df_needed['c_charge_degree']).codes\n",
    "\n",
    "# Releveling factors\n",
    "# df['age_factor'] = df['age_factor'].cat.reorder_categories(['Greater than 45', '25 - 45', 'Less than 25'], ordered=True)\n",
    "# df['race_factor'] = df['race_factor'].cat.reorder_categories(['African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other'], ordered=True)\n",
    "# df['gender_factor'] = df['gender_factor'].cat.reorder_categories(['Female', 'Male'], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df = df_needed[['priors_count', 'score_code', 'age_code', 'gender_code', 'race_code', 'crime_code', 'charge_degree_code']]\n",
    "out_df = df_needed[['two_year_recid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(torch.tensor(in_df.values), torch.tensor(out_df.values), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "\n",
    "        # Create a list of linear layers based on layer_sizes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu((layer(x)))\n",
    "        x = torch.sigmoid(self.layers[-1](x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_val)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 12\n",
    "hidden_size2 = 8\n",
    "op_size = 1\n",
    "layer_sizes = [input_size, hidden_size1, hidden_size2, op_size]\n",
    "model = SimpleClassifier(layer_sizes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    lab = []\n",
    "    x_val = []\n",
    "    out = []\n",
    "    start = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions = (outputs >= 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        end = start + len(labels)\n",
    "        x_val.append(X_val[start:end, :])\n",
    "        pred.append(predictions.flatten())\n",
    "        lab.append(labels.flatten())\n",
    "        out.append(outputs.flatten().float())\n",
    "        correct += (predictions == labels.view(-1, 1)).sum().item()\n",
    "        start = end\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.cat(pred, dim=0)\n",
    "lab = torch.cat(lab, dim=0)\n",
    "out = torch.cat(out, dim=0)\n",
    "tensor_list = []\n",
    "for i in range(len(x_val)):\n",
    "    for ip_ten in (x_val[i]):\n",
    "      tensor_list.append(np.array(ip_ten))\n",
    "tensor_list = np.array(tensor_list)\n",
    "pred = np.array(pred)\n",
    "lab = np.array(lab)\n",
    "out = np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_white_condition = (pred == 1) & (lab == 1) & (tensor_list[:, 5] == 1)\n",
    "tp_white = np.count_nonzero(tp_white_condition)\n",
    "print(tp_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_black_condition = (pred == 1) & (lab == 1) & (tensor_list[:, 5] == 0)\n",
    "tp_black = np.count_nonzero(tp_black_condition)\n",
    "print(tp_black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_white_condition = (pred == 1) & (lab == 0) & (tensor_list[:, 5] == 1)\n",
    "fp_white = np.count_nonzero(fp_white_condition)\n",
    "print(\"False positives in white:\", fp_white)\n",
    "p_white_condition = (lab == 1) & (tensor_list[:, 5] == 1)\n",
    "p_white = np.count_nonzero(p_white_condition)\n",
    "print(\"Total positives in white:\", p_white)\n",
    "print(\"False positive rate in white:\", fp_white/p_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_black_condition = (pred == 1) & (lab == 0) & (tensor_list[:, 5] == 0)\n",
    "fp_black = np.count_nonzero(fp_black_condition)\n",
    "print(\"False positives in black:\", fp_black)\n",
    "p_black_condition = (lab == 1) & (tensor_list[:, 5] == 1)\n",
    "p_black = np.count_nonzero(p_black_condition)\n",
    "print(\"Total positives in black:\", p_black)\n",
    "print(\"False positive rate in black:\", fp_black/p_black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['FP Black', 'FP White'], [fp_black/p_black, fp_white/p_white])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_black_0_label = out[(tensor_list[:, 5] == 0) & (lab == 0)]\n",
    "out_white_0_label = out[(tensor_list[:, 5] == 1) & (lab == 0)]\n",
    "# The mean value of the predicted label for black when the true value is 0 (Shows Blacks have a higher false positivity tendency)\n",
    "avg_b_l_0 = np.sum(out_black_0_label)/len(out_black_0_label)\n",
    "print(avg_b_l_0)\n",
    "# The mean value of the predicted label for white when the true value is 0 (Shows Whites have a lower false positivity tendency)\n",
    "avg_w_l_0 = np.sum(out_white_0_label)/len(out_white_0_label)\n",
    "print(avg_w_l_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_black_1_label = out[(tensor_list[:, 5] == 0) & (lab == 1)]\n",
    "out_white_1_label = out[(tensor_list[:, 5] == 1) & (lab == 1)]\n",
    "# The mean value of the predicted label for black when the true value is 1 (Shows Blacks have a lower false negative tendency)\n",
    "avg_b_l_1 = np.sum(out_black_1_label)/len(out_black_1_label)\n",
    "print(avg_b_l_1)\n",
    "# The mean value of the predicted label for black when the true value is 1 (Shows Whites have a higher false negative tendency)\n",
    "avg_w_l_1 = np.sum(out_white_1_label)/len(out_white_1_label)\n",
    "print(avg_w_l_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue', 'red', 'blue', 'red']\n",
    "plt.bar(['Avg(Pred|True=0,c=Black)', 'Avg(Pred|True=0,c=White)', 'Avg(Pred|True=1,c=Black)',\n",
    "        'Avg(Pred|True=1,c=White)'], [avg_b_l_0, avg_w_l_0, avg_b_l_1, avg_w_l_1], color=colors, alpha=0.5)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Marginal avg ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ghost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
